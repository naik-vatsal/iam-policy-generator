{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc38975e-8180-41fb-b36c-15f13f29374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SETTING UP GITHUB-READY PROJECT STRUCTURE\n",
      "============================================================\n",
      "\n",
      "1. Directories created\n",
      "\n",
      "2. Notebooks:\n",
      "   Copied 01_Dataset_Preparation.ipynb -> training/01_Dataset_Preparation.ipynb\n",
      "   Copied 02_Model_Training.ipynb -> training/02_Model_Training.ipynb\n",
      "\n",
      "3. Dataset:\n",
      "   Copied train.jsonl\n",
      "   Copied val.jsonl\n",
      "   Copied test.jsonl\n",
      "\n",
      "4. Report:\n",
      "   Copied report/technical_report.pdf -> docs/\n",
      "\n",
      "5. Results check:\n",
      "   ✅ results/config_a/final_model/adapter_model.safetensors\n",
      "   ✅ results/config_a/training_logs.json\n",
      "   ✅ results/config_b/training_logs.json\n",
      "   ✅ results/config_c/training_logs.json\n",
      "   ✅ results/config_d/training_logs.json\n",
      "   ✅ results/evaluation_metrics.json\n",
      "   ✅ results/evaluation_charts.png\n",
      "   ✅ results/hp_comparison_chart.png\n",
      "   ✅ results/error_analysis.json\n",
      "   ✅ results/zero_shot_results.json\n",
      "   ✅ results/few_shot_results.json\n",
      "   ✅ results/finetuned_results.json\n",
      "\n",
      "6. Source files check:\n",
      "   ❌ MISSING - upload this file! src/rag_pipeline.py\n",
      "   ❌ MISSING - upload this file! src/inference_pipeline.py\n",
      "   ❌ MISSING - upload this file! src/rag_app.py\n",
      "\n",
      "7. Root files check:\n",
      "   ✅ README.md\n",
      "   ✅ requirements.txt\n",
      "   ❌ MISSING - upload this file! .gitignore\n",
      "   ❌ MISSING - upload this file! .gitattributes\n",
      "   ❌ MISSING - upload this file! LICENSE\n",
      "\n",
      "============================================================\n",
      "FINAL STRUCTURE\n",
      "============================================================\n",
      "\n",
      "/home/naik.vat/Prompt_Engineering/\n",
      "├── README.md\n",
      "├── requirements.txt\n",
      "├── LICENSE\n",
      "├── .gitignore\n",
      "├── .gitattributes\n",
      "│\n",
      "├── src/\n",
      "│   ├── rag_pipeline.py          <- RAG-enhanced inference (MAIN)\n",
      "│   ├── inference_pipeline.py    <- Basic inference (no RAG)\n",
      "│   └── rag_app.py               <- Gradio web UI\n",
      "│\n",
      "├── training/\n",
      "│   ├── 01_Dataset_Preparation.ipynb\n",
      "│   └── 02_Model_Training.ipynb\n",
      "│\n",
      "├── data/processed/\n",
      "│   ├── train.jsonl (1,189)\n",
      "│   ├── val.jsonl (148)\n",
      "│   └── test.jsonl (151)\n",
      "│\n",
      "├── results/\n",
      "│   ├── config_a/final_model/    <- Best model (LoRA adapters)\n",
      "│   ├── config_*/training_logs.json\n",
      "│   ├── evaluation_metrics.json\n",
      "│   ├── evaluation_charts.png\n",
      "│   └── ...all result files...\n",
      "│\n",
      "└── docs/\n",
      "    └── technical_report.pdf\n",
      "\n",
      "============================================================\n",
      "NEXT STEPS\n",
      "============================================================\n",
      "\n",
      "1. Upload these files to your project root (if not already there):\n",
      "   - README.md, requirements.txt, .gitignore, .gitattributes, LICENSE\n",
      "\n",
      "2. Upload these to src/:\n",
      "   - rag_pipeline.py, inference_pipeline.py, rag_app.py\n",
      "\n",
      "3. Then run these git commands:\n",
      "\n",
      "   cd ~/Prompt_Engineering\n",
      "   git init\n",
      "   git add .\n",
      "   git commit -m \"Fine-tuned Mistral-7B for AWS IAM policy generation with RAG\"\n",
      "   git branch -M main\n",
      "   git remote add origin https://github.com/YOUR_USERNAME/iam-policy-generator.git\n",
      "   git push -u origin main\n",
      "\n",
      "   Note: If adapter_model.safetensors is too large for GitHub (>100MB),\n",
      "   either install Git LFS or add *.safetensors to .gitignore and\n",
      "   upload the model to HuggingFace Hub instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ~/Prompt_Engineering/setup_github.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f9b538-0866-45bc-89be-ecfe5a4cec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: /home/naik.vat/Prompt_Engineering/rag_pipeline.py\n",
      "Found: /home/naik.vat/Prompt_Engineering/inference_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "# Create src/ files by copying from where you uploaded them\n",
    "import os\n",
    "\n",
    "# Check where the files actually landed\n",
    "for name in [\"rag_pipeline.py\", \"inference_pipeline.py\", \"rag_app.py\"]:\n",
    "    for loc in [f\"/home/naik.vat/Prompt_Engineering/{name}\", \n",
    "                f\"/home/naik.vat/{name}\",\n",
    "                f\"/home/naik.vat/Prompt_Engineering/src/{name}\"]:\n",
    "        if os.path.exists(loc):\n",
    "            print(f\"Found: {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d60673c-ed2b-4a2e-999e-655d5b35a097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final check:\n",
      "  ✅ src/rag_pipeline.py\n",
      "  ✅ src/inference_pipeline.py\n",
      "  ✅ src/rag_app.py\n",
      "  ✅ .gitignore\n",
      "  ✅ .gitattributes\n",
      "  ✅ LICENSE\n",
      "  ✅ README.md\n",
      "  ✅ requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "proj = os.path.expanduser(\"~/Prompt_Engineering\")\n",
    "\n",
    "# Move src files to correct location\n",
    "os.makedirs(f\"{proj}/src\", exist_ok=True)\n",
    "shutil.copy2(f\"{proj}/rag_pipeline.py\", f\"{proj}/src/rag_pipeline.py\")\n",
    "shutil.copy2(f\"{proj}/inference_pipeline.py\", f\"{proj}/src/inference_pipeline.py\")\n",
    "\n",
    "# Create rag_app.py (was never uploaded)\n",
    "rag_app_code = '''\"\"\"\n",
    "RAG-Enhanced Gradio Demo\n",
    "Run with: python src/rag_app.py (from project root, requires GPU)\n",
    "\"\"\"\n",
    "import json\n",
    "import gradio as gr\n",
    "from rag_pipeline import RAGPolicyGenerator, detect_services, get_relevant_actions\n",
    "\n",
    "generator = RAGPolicyGenerator(\"results/config_a/final_model\")\n",
    "\n",
    "def generate_with_ui(description, max_tokens, temperature, use_rag):\n",
    "    services = detect_services(description)\n",
    "    services_str = \", \".join(services) if services else \"None detected\"\n",
    "    rag_context = get_relevant_actions(services, description) if use_rag and services else \"\"\n",
    "    result = generator.generate(description, max_tokens=int(max_tokens), temperature=temperature, use_rag=use_rag)\n",
    "    if result[\"valid_json\"]:\n",
    "        policy_str = json.dumps(result[\"policy\"], indent=2)\n",
    "        status = \"Valid IAM Policy\"\n",
    "    else:\n",
    "        policy_str = result[\"raw_output\"]\n",
    "        status = \"Invalid JSON - may need manual correction\"\n",
    "    info = f\"Services detected: {services_str}\\\\nRAG context: {\\\\'Injected\\\\' if rag_context else \\\\'None\\\\'}\"\n",
    "    return policy_str, status, info\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_with_ui,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Describe the IAM policy you need\", lines=3),\n",
    "        gr.Slider(minimum=128, maximum=1024, value=512, step=64, label=\"Max Tokens\"),\n",
    "        gr.Slider(minimum=0.0, maximum=1.0, value=0.1, step=0.05, label=\"Temperature\"),\n",
    "        gr.Checkbox(value=True, label=\"Enable RAG\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Code(label=\"Generated IAM Policy\", language=\"json\"),\n",
    "        gr.Textbox(label=\"Status\"),\n",
    "        gr.Textbox(label=\"RAG Info\"),\n",
    "    ],\n",
    "    title=\"AWS IAM Policy Generator (RAG-Enhanced)\",\n",
    "    examples=[\n",
    "        [\"Allow read-only access to S3 bucket named customer-data\", 512, 0.1, True],\n",
    "        [\"Allow a Lambda function to read from DynamoDB and write logs to CloudWatch\", 512, 0.1, True],\n",
    "        [\"Deny all S3 delete operations across all buckets\", 512, 0.1, True],\n",
    "    ],\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n",
    "'''\n",
    "\n",
    "with open(f\"{proj}/src/rag_app.py\", \"w\") as f:\n",
    "    f.write(rag_app_code)\n",
    "\n",
    "# Create .gitignore\n",
    "with open(f\"{proj}/.gitignore\", \"w\") as f:\n",
    "    f.write(\"\"\"__pycache__/\n",
    "*.py[cod]\n",
    ".ipynb_checkpoints/\n",
    "results/config_*/checkpoint-*/\n",
    "wandb/\n",
    "iam-finetuning/dataset/raw/\n",
    "iam-finetuning/dataset/synthetic_batches/\n",
    ".env\n",
    ".venv/\n",
    ".aws/\n",
    ".DS_Store\n",
    "*.tar.gz\n",
    "*.log\n",
    "\"\"\")\n",
    "\n",
    "# Create .gitattributes\n",
    "with open(f\"{proj}/.gitattributes\", \"w\") as f:\n",
    "    f.write(\"*.safetensors filter=lfs diff=lfs merge=lfs -text\\n\")\n",
    "\n",
    "# Create LICENSE\n",
    "with open(f\"{proj}/LICENSE\", \"w\") as f:\n",
    "    f.write(\"\"\"MIT License\n",
    "\n",
    "Copyright (c) 2026 Vatsal Naik\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\"\"\")\n",
    "\n",
    "# Verify everything\n",
    "print(\"Final check:\")\n",
    "for f in [\"src/rag_pipeline.py\", \"src/inference_pipeline.py\", \"src/rag_app.py\",\n",
    "          \".gitignore\", \".gitattributes\", \"LICENSE\", \"README.md\", \"requirements.txt\"]:\n",
    "    exists = os.path.exists(os.path.join(proj, f))\n",
    "    print(f\"  {'✅' if exists else '❌'} {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6d1f26-ff75-4ab7-b25e-bcadd1ff8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted rag_pipeline.py from root\n",
      "Deleted inference_pipeline.py from root\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "proj = os.path.expanduser(\"~/Prompt_Engineering\")\n",
    "for f in [\"rag_pipeline.py\", \"inference_pipeline.py\", \"rag_app.py\"]:\n",
    "    root_copy = os.path.join(proj, f)\n",
    "    if os.path.exists(root_copy):\n",
    "        os.remove(root_copy)\n",
    "        print(f\"Deleted {f} from root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc8c71f-2365-4110-bc1c-56b866f04396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n",
      "hint: is subject to change. To configure the initial branch name to use in all\n",
      "hint: of your new repositories, which will suppress this warning, call:\n",
      "hint: \n",
      "hint: \tgit config --global init.defaultBranch <name>\n",
      "hint: \n",
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n",
      "hint: 'development'. The just-created branch can be renamed via this command:\n",
      "hint: \n",
      "hint: \tgit branch -m <name>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/naik.vat/Prompt_Engineering/.git/\n",
      "On branch master\n",
      "\n",
      "No commits yet\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git rm --cached <file>...\" to unstage)\n",
      "\tnew file:   .gitattributes\n",
      "\tnew file:   .gitignore\n",
      "\tnew file:   01_Dataset_Preparation.ipynb\n",
      "\tnew file:   02_Model_Training.ipynb\n",
      "\tnew file:   Demo.ipynb\n",
      "\tnew file:   Demo2.ipynb\n",
      "\tnew file:   LICENSE\n",
      "\tnew file:   README.md\n",
      "\tnew file:   Untitled.ipynb\n",
      "\tnew file:   data/processed/test.jsonl\n",
      "\tnew file:   data/processed/train.jsonl\n",
      "\tnew file:   data/processed/val.jsonl\n",
      "\tnew file:   docs/technical_report.pdf\n",
      "\tnew file:   iam-finetuning/dataset/processed/all_validated_pairs.json\n",
      "\tnew file:   iam-finetuning/dataset/processed/test.jsonl\n",
      "\tnew file:   iam-finetuning/dataset/processed/train.jsonl\n",
      "\tnew file:   iam-finetuning/dataset/processed/val.jsonl\n",
      "\tnew file:   inference_app.py\n",
      "\tnew file:   rag_app.ipynb\n",
      "\tnew file:   report/technical_report.pdf\n",
      "\tnew file:   requirements.txt\n",
      "\tnew file:   results/comparison_examples.json\n",
      "\tnew file:   results/config_a/README.md\n",
      "\tnew file:   results/config_a/final_model/README.md\n",
      "\tnew file:   results/config_a/final_model/adapter_config.json\n",
      "\tnew file:   results/config_a/final_model/adapter_model.safetensors\n",
      "\tnew file:   results/config_a/final_model/tokenizer.json\n",
      "\tnew file:   results/config_a/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_a/final_model/training_args.bin\n",
      "\tnew file:   results/config_a/summary.json\n",
      "\tnew file:   results/config_a/training_logs.json\n",
      "\tnew file:   results/config_b/README.md\n",
      "\tnew file:   results/config_b/final_model/README.md\n",
      "\tnew file:   results/config_b/final_model/adapter_config.json\n",
      "\tnew file:   results/config_b/final_model/adapter_model.safetensors\n",
      "\tnew file:   results/config_b/final_model/tokenizer.json\n",
      "\tnew file:   results/config_b/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_b/final_model/training_args.bin\n",
      "\tnew file:   results/config_b/training_logs.json\n",
      "\tnew file:   results/config_c/README.md\n",
      "\tnew file:   results/config_c/final_model/README.md\n",
      "\tnew file:   results/config_c/final_model/adapter_config.json\n",
      "\tnew file:   results/config_c/final_model/adapter_model.safetensors\n",
      "\tnew file:   results/config_c/final_model/tokenizer.json\n",
      "\tnew file:   results/config_c/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_c/final_model/training_args.bin\n",
      "\tnew file:   results/config_c/training_logs.json\n",
      "\tnew file:   results/config_d/README.md\n",
      "\tnew file:   results/config_d/final_model/README.md\n",
      "\tnew file:   results/config_d/final_model/adapter_config.json\n",
      "\tnew file:   results/config_d/final_model/adapter_model.safetensors\n",
      "\tnew file:   results/config_d/final_model/tokenizer.json\n",
      "\tnew file:   results/config_d/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_d/final_model/training_args.bin\n",
      "\tnew file:   results/config_d/training_logs.json\n",
      "\tnew file:   results/error_analysis.json\n",
      "\tnew file:   results/evaluation_charts.png\n",
      "\tnew file:   results/evaluation_metrics.json\n",
      "\tnew file:   results/few_shot_results.json\n",
      "\tnew file:   results/finetuned_results.json\n",
      "\tnew file:   results/hp_comparison.json\n",
      "\tnew file:   results/hp_comparison_chart.png\n",
      "\tnew file:   results/zero_shot_results.json\n",
      "\tnew file:   setup_github.py\n",
      "\tnew file:   src/inference_pipeline.py\n",
      "\tnew file:   src/rag_app.py\n",
      "\tnew file:   src/rag_pipeline.py\n",
      "\tnew file:   test_model.ipynb\n",
      "\tnew file:   training/01_Dataset_Preparation.ipynb\n",
      "\tnew file:   training/02_Model_Training.ipynb\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   Untitled.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "git init\n",
    "git add .\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e441fa49-dbf3-45df-a4d2-7b35e211075b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "\n",
      "No commits yet\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git rm --cached <file>...\" to unstage)\n",
      "\tnew file:   .gitattributes\n",
      "\tnew file:   .gitignore\n",
      "\tnew file:   LICENSE\n",
      "\tnew file:   README.md\n",
      "\tnew file:   data/processed/test.jsonl\n",
      "\tnew file:   data/processed/train.jsonl\n",
      "\tnew file:   data/processed/val.jsonl\n",
      "\tnew file:   docs/technical_report.pdf\n",
      "\tnew file:   requirements.txt\n",
      "\tnew file:   results/comparison_examples.json\n",
      "\tnew file:   results/config_a/README.md\n",
      "\tnew file:   results/config_a/final_model/README.md\n",
      "\tnew file:   results/config_a/final_model/adapter_config.json\n",
      "\tnew file:   results/config_a/final_model/adapter_model.safetensors\n",
      "\tnew file:   results/config_a/final_model/tokenizer.json\n",
      "\tnew file:   results/config_a/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_a/final_model/training_args.bin\n",
      "\tnew file:   results/config_a/summary.json\n",
      "\tnew file:   results/config_a/training_logs.json\n",
      "\tnew file:   results/config_b/README.md\n",
      "\tnew file:   results/config_b/final_model/README.md\n",
      "\tnew file:   results/config_b/final_model/adapter_config.json\n",
      "\tnew file:   results/config_b/final_model/tokenizer.json\n",
      "\tnew file:   results/config_b/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_b/final_model/training_args.bin\n",
      "\tnew file:   results/config_b/training_logs.json\n",
      "\tnew file:   results/config_c/README.md\n",
      "\tnew file:   results/config_c/final_model/README.md\n",
      "\tnew file:   results/config_c/final_model/adapter_config.json\n",
      "\tnew file:   results/config_c/final_model/tokenizer.json\n",
      "\tnew file:   results/config_c/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_c/final_model/training_args.bin\n",
      "\tnew file:   results/config_c/training_logs.json\n",
      "\tnew file:   results/config_d/README.md\n",
      "\tnew file:   results/config_d/final_model/README.md\n",
      "\tnew file:   results/config_d/final_model/adapter_config.json\n",
      "\tnew file:   results/config_d/final_model/tokenizer.json\n",
      "\tnew file:   results/config_d/final_model/tokenizer_config.json\n",
      "\tnew file:   results/config_d/final_model/training_args.bin\n",
      "\tnew file:   results/config_d/training_logs.json\n",
      "\tnew file:   results/error_analysis.json\n",
      "\tnew file:   results/evaluation_charts.png\n",
      "\tnew file:   results/evaluation_metrics.json\n",
      "\tnew file:   results/few_shot_results.json\n",
      "\tnew file:   results/finetuned_results.json\n",
      "\tnew file:   results/hp_comparison.json\n",
      "\tnew file:   results/hp_comparison_chart.png\n",
      "\tnew file:   results/zero_shot_results.json\n",
      "\tnew file:   src/inference_pipeline.py\n",
      "\tnew file:   src/rag_app.py\n",
      "\tnew file:   src/rag_pipeline.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "\n",
    "# Unstage everything first\n",
    "git reset\n",
    "\n",
    "# Remove junk files from tracking\n",
    "git rm --cached -f Demo.ipynb Demo2.ipynb Untitled.ipynb test_model.ipynb rag_app.ipynb 2>/dev/null\n",
    "git rm --cached -f setup_github.py inference_app.py 2>/dev/null\n",
    "git rm --cached -f 01_Dataset_Preparation.ipynb 02_Model_Training.ipynb 2>/dev/null\n",
    "git rm --cached -rf iam-finetuning/ report/ 2>/dev/null\n",
    "git rm --cached -f results/config_b/final_model/adapter_model.safetensors 2>/dev/null\n",
    "git rm --cached -f results/config_c/final_model/adapter_model.safetensors 2>/dev/null\n",
    "git rm --cached -f results/config_d/final_model/adapter_model.safetensors 2>/dev/null\n",
    "\n",
    "# Add junk files to .gitignore\n",
    "cat >> .gitignore << 'EOF'\n",
    "Demo.ipynb\n",
    "Demo2.ipynb\n",
    "Untitled.ipynb\n",
    "test_model.ipynb\n",
    "rag_app.ipynb\n",
    "setup_github.py\n",
    "inference_app.py\n",
    "01_Dataset_Preparation.ipynb\n",
    "02_Model_Training.ipynb\n",
    "iam-finetuning/\n",
    "report/\n",
    "results/config_b/final_model/adapter_model.safetensors\n",
    "results/config_c/final_model/adapter_model.safetensors\n",
    "results/config_d/final_model/adapter_model.safetensors\n",
    "EOF\n",
    "\n",
    "# Now add only what we want\n",
    "git add .\n",
    "git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0831759f-7bbd-4f7d-ade3-e87af6e3e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main (root-commit) 9f2d04e] Fine-tuned Mistral-7B for AWS IAM policy generation with RAG\n",
      " 51 files changed, 1111389 insertions(+)\n",
      " create mode 100644 .gitattributes\n",
      " create mode 100644 .gitignore\n",
      " create mode 100644 LICENSE\n",
      " create mode 100644 README.md\n",
      " create mode 100644 data/processed/test.jsonl\n",
      " create mode 100644 data/processed/train.jsonl\n",
      " create mode 100644 data/processed/val.jsonl\n",
      " create mode 100644 docs/technical_report.pdf\n",
      " create mode 100644 requirements.txt\n",
      " create mode 100644 results/comparison_examples.json\n",
      " create mode 100644 results/config_a/README.md\n",
      " create mode 100644 results/config_a/final_model/README.md\n",
      " create mode 100644 results/config_a/final_model/adapter_config.json\n",
      " create mode 100644 results/config_a/final_model/adapter_model.safetensors\n",
      " create mode 100644 results/config_a/final_model/tokenizer.json\n",
      " create mode 100644 results/config_a/final_model/tokenizer_config.json\n",
      " create mode 100644 results/config_a/final_model/training_args.bin\n",
      " create mode 100644 results/config_a/summary.json\n",
      " create mode 100644 results/config_a/training_logs.json\n",
      " create mode 100644 results/config_b/README.md\n",
      " create mode 100644 results/config_b/final_model/README.md\n",
      " create mode 100644 results/config_b/final_model/adapter_config.json\n",
      " create mode 100644 results/config_b/final_model/tokenizer.json\n",
      " create mode 100644 results/config_b/final_model/tokenizer_config.json\n",
      " create mode 100644 results/config_b/final_model/training_args.bin\n",
      " create mode 100644 results/config_b/training_logs.json\n",
      " create mode 100644 results/config_c/README.md\n",
      " create mode 100644 results/config_c/final_model/README.md\n",
      " create mode 100644 results/config_c/final_model/adapter_config.json\n",
      " create mode 100644 results/config_c/final_model/tokenizer.json\n",
      " create mode 100644 results/config_c/final_model/tokenizer_config.json\n",
      " create mode 100644 results/config_c/final_model/training_args.bin\n",
      " create mode 100644 results/config_c/training_logs.json\n",
      " create mode 100644 results/config_d/README.md\n",
      " create mode 100644 results/config_d/final_model/README.md\n",
      " create mode 100644 results/config_d/final_model/adapter_config.json\n",
      " create mode 100644 results/config_d/final_model/tokenizer.json\n",
      " create mode 100644 results/config_d/final_model/tokenizer_config.json\n",
      " create mode 100644 results/config_d/final_model/training_args.bin\n",
      " create mode 100644 results/config_d/training_logs.json\n",
      " create mode 100644 results/error_analysis.json\n",
      " create mode 100644 results/evaluation_charts.png\n",
      " create mode 100644 results/evaluation_metrics.json\n",
      " create mode 100644 results/few_shot_results.json\n",
      " create mode 100644 results/finetuned_results.json\n",
      " create mode 100644 results/hp_comparison.json\n",
      " create mode 100644 results/hp_comparison_chart.png\n",
      " create mode 100644 results/zero_shot_results.json\n",
      " create mode 100644 src/inference_pipeline.py\n",
      " create mode 100644 src/rag_app.py\n",
      " create mode 100644 src/rag_pipeline.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(gnome-ssh-askpass:1936057): Gtk-WARNING **: 15:58:18.417: cannot open display: \n",
      "error: unable to read askpass response from '/usr/libexec/openssh/gnome-ssh-askpass'\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cd ~/Prompt_Engineering\\ngit config user.email \"naik.vat@northeastern.edu\"\\ngit config user.name \"Vatsal Naik\"\\ngit commit -m \"Fine-tuned Mistral-7B for AWS IAM policy generation with RAG\"\\ngit branch -M main\\ngit push -u origin main\\n'' returned non-zero exit status 128.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbash\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd ~/Prompt_Engineering\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit config user.email \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaik.vat@northeastern.edu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit config user.name \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVatsal Naik\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit commit -m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuned Mistral-7B for AWS IAM policy generation with RAG\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit branch -M main\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mgit push -u origin main\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshebang(line, cell)\n",
      "File \u001b[0;32m/shared/EL9/explorer/anaconda3/2024.06/lib/python3.12/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cd ~/Prompt_Engineering\\ngit config user.email \"naik.vat@northeastern.edu\"\\ngit config user.name \"Vatsal Naik\"\\ngit commit -m \"Fine-tuned Mistral-7B for AWS IAM policy generation with RAG\"\\ngit branch -M main\\ngit push -u origin main\\n'' returned non-zero exit status 128."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "git config user.email \"naik.vat@northeastern.edu\"\n",
    "git config user.name \"Vatsal Naik\"\n",
    "git commit -m \"Fine-tuned Mistral-7B for AWS IAM policy generation with RAG\"\n",
    "git branch -M main\n",
    "git push -u origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1a3da9-09d9-4fad-88b1-fb3cfe0bd777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL cleaned - token removed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "git remote set-url origin https://github.com/naik-vatsal/iam-policy-generator.git\n",
    "echo \"URL cleaned - token removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41775dde-5693-44ae-a42d-d7e58325d6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm 'results/config_a/final_model/adapter_model.safetensors'\n",
      "[main 01fb696] Remove large model file, add to gitignore\n",
      " 2 files changed, 1 insertion(+)\n",
      " delete mode 100644 results/config_a/final_model/adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "echo \"*.safetensors\" >> .gitignore\n",
    "git rm --cached results/config_a/final_model/adapter_model.safetensors\n",
    "git add .gitignore\n",
    "git commit -m \"Remove large model file, add to gitignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4bcaf13-37a0-4841-aad1-d4ae80f246e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token removed from config\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Prompt_Engineering\n",
    "git remote set-url origin https://github.com/naik-vatsal/iam-policy-generator.git\n",
    "echo \"Token removed from config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338a9bd-b418-468d-86c0-24023345dceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
